from pyexpat import model
import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_teddynote.prompts import load_prompt
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_teddynote import logging
from dotenv import load_dotenv
import os
from retriever import create_retriever
from llama_cpp import Llama

# API KEY ì •ë³´ë¡œë“œ
load_dotenv()

# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.
logging.langsmith("[Project] PDF RAG")

st.title("Local ëª¨ë¸ ê¸°ë°˜ RAG ğŸ’¬")


# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥
def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)


# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€
def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))


def format_doc(document_list):
    return "\n\n".join([doc.page_content for doc in document_list])


# ì²´ì¸ ìƒì„±
def create_chain(retriever, model_name="OpenAI"):
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = load_prompt(
        "D:\\LLM\prompts\\pdf-rag.yaml",
        encoding="utf-8",
    )

    if model_name == "EEVE":
        llm = Llama(
            model_path="D:\\LLM_MODEL\\EEVE-Korean-Instruct-10.8B-v1.0-Q4_1.gguf",
            n_ctx=4096,  # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¦ê°€
            # max_tokens=2048,  # ìµœëŒ€ í† í° ìˆ˜ ì¦ê°€
            temperature=0,  # ìƒì„±ì˜ ë‹¤ì–‘ì„± ì¡°ì ˆ
        )

    elif model_name == "Qwen":
        llm = Llama(
            model_path="D:\\LLM_MODEL\\Qwen2.5-7B-Instruct-kowiki-qa-Q4_0.gguf",
            n_ctx=4096,  # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¦ê°€
            max_tokens=2048,  # ìµœëŒ€ í† í° ìˆ˜ ì¦ê°€
            temperature=0,  # ìƒì„±ì˜ ë‹¤ì–‘ì„± ì¡°ì ˆ
        )

    def process_llm_response(response):
        """LLM ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
        if hasattr(response, "content"):
            return response.content
        elif isinstance(response, dict):
            # Llama ëª¨ë¸ì˜ ì‘ë‹µ í˜•ì‹ ì²˜ë¦¬
            if "choices" in response:
                try:
                    return response["choices"][0]["text"]
                except (KeyError, IndexError):
                    return ""
            # ë‹¤ë¥¸ í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬ ì‘ë‹µ ì²˜ë¦¬
            return (
                response.get("content", "") or response.get("text", "") or str(response)
            )
        return str(response)

    def get_context(question: str):
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
        docs = retriever.get_relevant_documents(question)
        return "\n\n".join(doc.page_content for doc in docs)

    chain = (
        {"question": RunnablePassthrough(), "context": lambda x: get_context(x)}
        | prompt
        | (lambda x: x.to_string())  # Convert PromptValue to string
        | llm
        | process_llm_response
        | StrOutputParser()
    )
    return chain


# ì‚¬ì´ë“œë°” ìƒì„±
with st.sidebar:
    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")

    # ëª¨ë¸ ì„ íƒ ë©”ë‰´
    selected_model = st.selectbox("LLM ì„ íƒ", ["EEVE", "Qwen"], index=0)


# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ
if "messages" not in st.session_state:
    # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥
    st.session_state["messages"] = []

if "chain" not in st.session_state:
    # DBì—ì„œ retriever ìƒì„±
    retriever = create_retriever()
    # ì‚¬ì´ë“œë°”ë¥¼ í†µí•´ ìƒì„±í•œ ëª¨ë¸ë¡œ chain ìƒì„±
    chain = create_chain(retriever, model_name=selected_model)
    st.session_state["chain"] = chain


# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...
if clear_btn:
    st.session_state["messages"] = []

# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

# ì‚¬ìš©ìì˜ ì…ë ¥
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­
warning_msg = st.empty()
# st.session_state["chain"] = chain

# ë§Œì•½ì— ì‚¬ìš©ì ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...
if user_input:
    # chain ì„ ìƒì„±
    chain = st.session_state["chain"]

    # ì‚¬ìš©ìì˜ ì…ë ¥
    st.chat_message("user").write(user_input)
    # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
    response = chain.stream(user_input)

    with st.chat_message("assistant"):
        # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
        container = st.empty()

        ai_answer = ""
        for token in response:
            ai_answer += token
            container.markdown(ai_answer)

    # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•œë‹¤.
    add_message("user", user_input)
    add_message("assistant", ai_answer)
